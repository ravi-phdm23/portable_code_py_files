"""
FastAPI application with route definitions only.
All business logic delegated to services.
All persistence delegated to repositories.
"""

import os
# Fix for PyTorch DLL initialization error (WinError 1114)
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

# Load environment variables from .env file
from dotenv import load_dotenv
load_dotenv()
print(f">>> .env loaded. OPENAI_API_KEY present: {bool(os.environ.get('OPENAI_API_KEY'))}")

from fastapi import FastAPI, UploadFile, HTTPException, Depends
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from sqlalchemy.orm import Session
from fastapi.staticfiles import StaticFiles
import logging
from logging import handlers

# Configuration
from config import IMAGES_DIR, ALLOWED_ORIGINS, get_images_dir, get_reports_dir, get_logs_dir
from settings_manager import SettingsManager

# Schemas
from schemas.request_models import (
    FeatureImportanceRequest,
    SMOTERequest,
    RunModelsRequest,
    SHAPAnalysisRequest,
    StatisticalTestRequest,
    ValidatedExplanationRequest,
    LabelRandomizationRequest,
    LocalSHAPRequest
)

# Services
from services.eda_service import (
    generate_column_summary,
    generate_describe_table,
    generate_sample_data,
    generate_target_distribution
)
from services.plotting_service import (
    generate_pairplot,
    generate_correlation_heatmap,
    generate_feature_importance_chart
)
from services.feature_importance_service import calculate_feature_importance
from services.smote_service import apply_smote_analysis
from services.model_service import run_models
# Import SHAP service with error handling - SHAP has PyTorch dependency that may fail on some systems
try:
    from services.shap_service import run_shap_analysis
    SHAP_AVAILABLE = True
except (ImportError, OSError) as e:
    SHAP_AVAILABLE = False
    logger = logging.getLogger(__name__)
    logger.warning(f"SHAP service unavailable (PyTorch dependency issue): {e}")
    def run_shap_analysis(*args, **kwargs):
        """Stub implementation when SHAP is unavailable"""
        raise RuntimeError("SHAP analysis is not available due to PyTorch initialization failure. "
                          "Please reinstall PyTorch or disable GPU support.")

# Persistence
from persistence.dataset_repository import (
    save_csv_data,
    load_csv_data,
    get_dataset_metadata,
    check_dataset_exists,
    list_all_datasets,
    get_most_recent_filename,
    delete_dataset,
    get_dataset_quality_summary
)
from persistence.analysis_repository import (
    save_pandas_result,
    load_pandas_result,
    save_image_path,
    load_image_path,
    save_feature_importance_result,
    load_all_feature_importance_results,
    check_feature_importance_cached,
    delete_dataset_images,
    save_smote_result,
    load_smote_result
)
from persistence.model_repository import (
    save_model_result,
    load_model_results,
    list_all_model_runs,
    get_model_run_details,
    get_model_run_details_by_id,
    delete_model_results,
    delete_model_run,
    save_group_best_result,
    save_benchmark_result,
    load_group_bests,
    load_benchmark_for_run
)
from persistence.shap_repository import (
    save_shap_result,
    load_shap_result,
    check_shap_cached,
    delete_shap_result
)
from persistence.report_repository import (
    save_report,
    get_report_by_id,
    list_all_reports,
    delete_report,
    report_to_dict
)
from persistence.db import get_db, get_db_session

# Utils
from utils.validation import detect_target_column
from utils.dataframe_utils import parse_csv_file


# Initialize FastAPI app
app = FastAPI()

# Logging setup
LOGS_DIR = get_logs_dir()
os.makedirs(LOGS_DIR, exist_ok=True)

logging.basicConfig(level=logging.INFO,
                    format="%(asctime)s %(levelname)s %(name)s: %(message)s")
# root logger
logger = logging.getLogger()
# file handler - rotating
file_handler = handlers.RotatingFileHandler(os.path.join(LOGS_DIR, 'app.log'), maxBytes=5_000_000, backupCount=3)
file_handler.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
logger.addHandler(file_handler)


@app.on_event("startup")
def _initialize_settings():
    """
    Initialize settings on application startup.
    Ensures settings file exists with defaults if missing.
    Creates required directories.
    """
    try:
        # Ensure /app/data exists for Docker environments
        data_dir = '/app/data'
        if os.path.exists('/app') and not os.path.exists(data_dir):
            try:
                os.makedirs(data_dir, exist_ok=True)
                logger = logging.getLogger(__name__)
                logger.info(f"Created Docker data directory: {data_dir}")
            except Exception as mkdir_err:
                logger = logging.getLogger(__name__)
                logger.warning(f"Could not create {data_dir}: {mkdir_err}")

        settings_mgr = SettingsManager.get_instance()
        settings = settings_mgr.load_settings()

        # Log configured paths
        logger = logging.getLogger(__name__)
        logger.info("=" * 60)
        logger.info("Application Settings Initialized")
        logger.info("=" * 60)
        logger.info(f"Database: {settings['paths']['database_path']}")
        logger.info(f"Images:   {settings['paths']['images_dir']}")
        logger.info(f"Reports:  {settings['paths']['reports_dir']}")
        logger.info(f"Logs:     {settings['paths']['logs_dir']}")
        logger.info("=" * 60)

    except Exception as e:
        logger = logging.getLogger(__name__)
        logger.error(f"Error initializing settings: {str(e)}")
        logger.error("Continuing with default configuration")


@app.on_event("startup")
def _configure_logging_on_startup():
    """Ensure logging handler exists in worker process (uvicorn reload spawns subprocess)."""
    root = logging.getLogger()
    root.setLevel(logging.INFO)  # Changed from DEBUG to INFO to reduce noise
    
    # Suppress chatty libraries
    logging.getLogger('matplotlib').setLevel(logging.WARNING)
    logging.getLogger('PIL').setLevel(logging.WARNING)
    
    logpath = os.path.join(LOGS_DIR, 'app.log')
    # add rotating file handler if not already present
    found = False
    for h in root.handlers:
        try:
            if getattr(h, 'baseFilename', None) and os.path.abspath(h.baseFilename) == os.path.abspath(logpath):
                found = True
                break
        except Exception:
            continue

    if not found:
        fh = handlers.RotatingFileHandler(logpath, maxBytes=5_000_000, backupCount=3)
        fh.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
        root.addHandler(fh)

# CORS configuration
app.add_middleware(
    CORSMiddleware,
    allow_origins=ALLOWED_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Mount images directory for static file serving
# Images can be mounted early since they use a specific path (/images)
images_dir = get_images_dir()
os.makedirs(images_dir, exist_ok=True)
app.mount("/images", StaticFiles(directory=images_dir), name="images")

# NOTE: React frontend static files will be mounted at the END of this file
# after ALL API routes are defined. This ensures API routes take precedence.


@app.post("/upload_csv")
async def upload_csv(file: UploadFile, overwrite: bool = False):
    """
    Accepts a CSV file and checks if it already exists.
    Returns minimal response - no data displayed until explicitly requested.
    """
    if not file:
        raise HTTPException(status_code=400, detail="No file provided")

    if not file.filename.endswith('.csv'):
        raise HTTPException(status_code=400, detail="File must be a CSV")

    filename = file.filename

    # Check if file already exists
    file_exists = check_dataset_exists(filename)

    if file_exists and not overwrite:
        return {
            "file_exists": True,
            "filename": filename,
            "uploaded": False,
            "message": f"File '{filename}' already exists. Set overwrite=true to replace it."
        }

    # Read and parse CSV
    try:
        content = await file.read()
        df = parse_csv_file(content)
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Invalid CSV file: {str(e)}")

    # Validate target column
    target_column = detect_target_column(df)
    if target_column is None:
        raise HTTPException(
            status_code=400,
            detail="Required column 'target' not found in CSV. The dataset must contain a column named exactly 'target' (lowercase)."
        )

    # Save CSV data
    try:
        metadata = save_csv_data(filename, df, target_column)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Database error: {str(e)}")

    return {
        "file_exists": False,
        "filename": filename,
        "uploaded": True,
        "message": f"File '{filename}' uploaded successfully. Total rows: {metadata['total_rows']}, Total columns: {metadata['total_columns']}"
    }


@app.get("/get_data_details")
async def get_data_details(filename: str = None):
    """
    Retrieve comprehensive data exploration results.
    If filename not provided, uses most recent upload.
    """
    try:
        # Determine which file to analyze
        if not filename:
            filename = get_most_recent_filename()
            if not filename:
                raise HTTPException(status_code=404, detail="No CSV files found in database")

        # Load CSV data
        df = load_csv_data(filename)
        if df is None:
            raise HTTPException(status_code=404, detail=f"No CSV data found for file: {filename}")

        # Re-validate target column exists
        if 'target' not in df.columns:
            raise HTTPException(
                status_code=400,
                detail=f"Dataset '{filename}' does not contain required 'target' column. This dataset is invalid."
            )

        # Get metadata
        metadata = get_dataset_metadata(filename)

        result = {
            "filename": filename,
            "total_rows": metadata['total_rows'],
            "total_columns": metadata['total_columns'],
            "target_column": "target"
        }

        # Generate or retrieve sample data
        cached_sample = load_pandas_result(filename, "sample_data")
        if cached_sample:
            result["sample_data"] = cached_sample
        else:
            sample_data = generate_sample_data(df)
            save_pandas_result(filename, "sample_data", sample_data)
            result["sample_data"] = sample_data

        # Generate or retrieve column summary
        cached_summary = load_pandas_result(filename, "column_summary")
        if cached_summary:
            result["column_summary"] = cached_summary
        else:
            column_summary = generate_column_summary(df)
            save_pandas_result(filename, "column_summary", column_summary)
            result["column_summary"] = column_summary

        # Generate or retrieve statistical summary
        cached_describe = load_pandas_result(filename, "describe")
        if cached_describe:
            result["describe"] = cached_describe
        else:
            describe_table = generate_describe_table(df)
            if describe_table:
                save_pandas_result(filename, "describe", describe_table)
                result["describe"] = describe_table

        # Generate or retrieve pairplot
        cached_pairplot = load_image_path(filename, "pairplot")
        if cached_pairplot:
            result["pairplot_path"] = cached_pairplot
        else:
            pairplot_path = generate_pairplot(df, filename)
            if pairplot_path:
                save_image_path(filename, "pairplot", pairplot_path)
                result["pairplot_path"] = pairplot_path

        # Generate or retrieve correlation heatmap
        cached_heatmap = load_image_path(filename, "correlation_heatmap")
        if cached_heatmap:
            result["heatmap_path"] = cached_heatmap
        else:
            heatmap_path = generate_correlation_heatmap(df, filename)
            if heatmap_path:
                save_image_path(filename, "correlation_heatmap", heatmap_path)
                result["heatmap_path"] = heatmap_path

        # Generate or retrieve target distribution
        cached_target_dist = load_pandas_result(filename, "target_distribution")
        if cached_target_dist:
            result["target_distribution"] = cached_target_dist
        else:
            target_dist = generate_target_distribution(df)
            if target_dist:
                save_pandas_result(filename, "target_distribution", target_dist)
                result["target_distribution"] = target_dist

        # Generate or retrieve dataset quality summary (Table X)
        quality_summary = get_dataset_quality_summary(filename)
        if quality_summary:
            result["dataset_quality_summary"] = quality_summary
        else:
            # Fallback: generate if missing (for datasets uploaded before this feature)
            from services.dataset_quality import build_dataset_quality_summary
            from persistence.dataset_quality_repo import save_dataset_quality_summary

            quality_summary = build_dataset_quality_summary(df)

            db_session = get_db_session()
            try:
                save_dataset_quality_summary(db_session, filename, quality_summary)
                db_session.commit()
                result["dataset_quality_summary"] = quality_summary
            except Exception as e:
                db_session.rollback()
                print(f"Warning: Failed to save quality summary: {e}")
            finally:
                db_session.close()

        # Check if feature importance exists and load it
        if check_feature_importance_cached(filename):
            cached_fi_results = load_all_feature_importance_results(filename)
            fi_chart_path = load_image_path(filename, "feature_importance")
            
            result["feature_importance"] = {
                "rf_table": cached_fi_results.get("rf_table"),
                "lr_table": cached_fi_results.get("lr_table"),
                "merged_table": cached_fi_results.get("merged_table"),
                "metadata": cached_fi_results.get("metadata"),
                "chart_path": fi_chart_path
            }

        # Check if SMOTE results exist and load it
        cached_smote = load_smote_result(filename)
        if cached_smote:
            result["smote_result"] = cached_smote

        # REMOVED: Model result injection (was causing phantom/duplicate models in View mode)
        # View mode must use dedicated /model-results endpoint instead
        # Only indicate whether results exist, do not inject them here
        model_results = load_model_results(filename)
        result["has_model_results"] = bool(model_results and len(model_results) > 0)
        # Use NaN-safe encoder for the response to prevent JSON serialization errors
        from persistence.analysis_repository import NaNSafeEncoder
        import json
        return JSONResponse(content=json.loads(json.dumps(result, cls=NaNSafeEncoder)))
    
    except HTTPException:
        raise
    except Exception as e:
        logger.exception(f"Error in get_data_details for {filename}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")


@app.get("/list_datasets")
async def list_datasets():
    """List all uploaded CSV files with metadata"""
    datasets = list_all_datasets()
    return {"datasets": datasets}


@app.get("/get_statistics")
async def get_statistics(filename: str = None):
    """
    Legacy endpoint for backward compatibility.
    Returns only statistical summary.
    """
    if not filename:
        filename = get_most_recent_filename()
        if not filename:
            raise HTTPException(status_code=404, detail="No CSV files found")

    # Load CSV data
    df = load_csv_data(filename)
    if df is None:
        raise HTTPException(status_code=404, detail=f"No CSV data found for file: {filename}")

    # Check for cached result
    cached_describe = load_pandas_result(filename, "describe")
    if cached_describe:
        return {
            "filename": filename,
            "describe": cached_describe
        }

    # Generate describe table
    describe_table = generate_describe_table(df)
    if describe_table:
        save_pandas_result(filename, "describe", describe_table)
        return {
            "filename": filename,
            "describe": describe_table
        }

    raise HTTPException(status_code=400, detail="No numeric columns found for statistical analysis")


@app.post("/feature-importance")
async def calculate_feature_importance_endpoint(request: FeatureImportanceRequest):
    """
    Calculate feature importance for a given dataset.
    Returns RF table, LR table, merged table, and visualization.
    """
    filename = request.filename

    # Check if results already cached
    if check_feature_importance_cached(filename):
        cached_results = load_all_feature_importance_results(filename)
        cached_image = load_image_path(filename, "feature_importance")

        return {
            "filename": filename,
            "rf_table": cached_results["rf_table"],
            "lr_table": cached_results["lr_table"],
            "merged_table": cached_results["merged_table"],
            "metadata": cached_results["metadata"],
            "chart_path": cached_image
        }

    # Load data from database
    df = load_csv_data(filename)
    if df is None:
        raise HTTPException(status_code=404, detail=f"No CSV data found for file: {filename}")

    # Calculate feature importance
    try:
        fi_results = calculate_feature_importance(df)

        rf_table = fi_results['rf_table']
        lr_table = fi_results['lr_table']
        merged_table = fi_results['merged_table']

        # Convert to JSON-serializable format
        rf_table_json = rf_table.to_dict(orient='records')
        lr_table_json = lr_table.to_dict(orient='records')
        merged_table_json = merged_table.to_dict(orient='records')

        metadata = {
            "total_features": len(fi_results['feature_names']),
            "feature_names": fi_results['feature_names']
        }

        # Generate chart
        chart_path = generate_feature_importance_chart(merged_table, filename)

        # Store results in database
        try:
            save_feature_importance_result(filename, "rf_table", rf_table_json)
            save_feature_importance_result(filename, "lr_table", lr_table_json)
            save_feature_importance_result(filename, "merged_table", merged_table_json)
            save_feature_importance_result(filename, "metadata", metadata)

            if chart_path:
                save_image_path(filename, "feature_importance", chart_path)
        except Exception as e:
            print(f"Warning: Failed to store feature importance results: {str(e)}")

        return {
            "filename": filename,
            "rf_table": rf_table_json,
            "lr_table": lr_table_json,
            "merged_table": merged_table_json,
            "metadata": metadata,
            "chart_path": chart_path
        }

    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error calculating feature importance: {str(e)}")


@app.delete("/delete_dataset")
async def delete_dataset_endpoint(filename: str):
    """
    Delete a dataset and all associated data (database, images, and models).
    """
    # Delete from database
    deleted = delete_dataset(filename)
    if not deleted:
        raise HTTPException(status_code=404, detail=f"Dataset '{filename}' not found")

    # Delete image files
    try:
        delete_dataset_images(filename)
    except Exception as e:
        print(f"Warning: Error deleting images: {str(e)}")

    # Delete model files
    try:
        from services.model_persistence import delete_dataset_models
        delete_dataset_models(filename)
    except Exception as e:
        print(f"Warning: Error deleting models: {str(e)}")

    # Delete statistical test results
    try:
        from services.statistical_tests_service import delete_statistical_test_results
        count = delete_statistical_test_results(filename)
        if count > 0:
            print(f"Deleted {count} statistical test results for '{filename}'")
    except Exception as e:
        print(f"Warning: Error deleting statistical test results: {str(e)}")

    # Delete validated commentary
    try:
        from persistence.validated_commentary_repository import delete_validated_commentary
        count = delete_validated_commentary(filename)
        if count > 0:
            print(f"Deleted {count} validated commentary record(s) for '{filename}'")
    except Exception as e:
        print(f"Warning: Error deleting validated commentary: {str(e)}")

    # Delete label randomization results
    try:
        from persistence.db import get_engine
        engine = get_engine()
        with engine.connect() as conn:
            raw_conn = conn.connection
            cursor = raw_conn.cursor()
            cursor.execute('DELETE FROM label_randomization_results WHERE filename = ?', (filename,))
            count = cursor.rowcount
            raw_conn.commit()
            if count > 0:
                print(f"Deleted {count} label randomization result(s) for '{filename}'")
    except Exception as e:
        print(f"Warning: Error deleting label randomization results: {str(e)}")

    return {
        "success": True,
        "message": f"Dataset '{filename}' and all associated data deleted successfully"
    }


@app.post("/apply-smote")
async def apply_smote_endpoint(request: SMOTERequest):
    """
    Apply SMOTE analysis on selected features.
    Returns class distributions before and after SMOTE (if applied).
    """
    filename = request.filename
    selected_features = request.selected_features
    apply_smote = request.apply_smote

    # Check if result is cached
    cached_result = load_smote_result(filename)
    if cached_result and \
       cached_result.get('selected_features') == selected_features and \
       cached_result.get('applied') == apply_smote:
        return cached_result

    # Load data from database
    df = load_csv_data(filename)
    if df is None:
        raise HTTPException(status_code=404, detail=f"No CSV data found for file: {filename}")

    # Apply SMOTE analysis
    try:
        result = apply_smote_analysis(df, selected_features, apply_smote)

        # Store result in database
        try:
            save_smote_result(filename, result)
        except Exception as e:
            print(f"Warning: Failed to store SMOTE result: {str(e)}")

        return result

    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error applying SMOTE: {str(e)}")


@app.post("/run-models")
async def run_models_endpoint(request: RunModelsRequest):
    """
    Execute LR and LR-Reg models on the dataset with selected features.
    Returns metrics for both models.
    """
    filename = request.filename
    selected_features = request.selected_features
    target = request.target
    use_smote = request.use_smote

    # Load data from database
    df = load_csv_data(filename)
    if df is None:
        raise HTTPException(status_code=404, detail=f"No CSV data found for file: {filename}")

    try:
        result = run_models(
            df,
            selected_features,
            target,
            use_smote,
            selected_models=request.selected_models,  # Legacy support
            selected_model_groups=request.selected_model_groups  # New format
        )

        # CRITICAL: Delete ALL previous model results for this dataset BEFORE saving new ones
        # This ensures the database reflects ONLY the current run, not historical accumulation
        deleted_count = delete_model_results(filename)
        print(f"Deleted {deleted_count} old model result(s) for dataset '{filename}'")

        # Also delete SHAP results since they're based on model results and may be outdated
        shap_deleted_count = delete_shap_result(filename)
        if shap_deleted_count > 0:
            print(f"Deleted {shap_deleted_count} SHAP result(s) for dataset '{filename}' (invalidated by model rerun)")

        # Use a common timestamp for all models in this run so they can be grouped together
        from datetime import datetime as _dt
        run_timestamp = _dt.now()

        # Store each model result in database
        # Use all_models if available (new format), otherwise fall back to results (legacy format)
        models_to_save = result.get('all_models', result.get('results', []))
        
        print(f"DEBUG: Attempting to save {len(models_to_save)} model results...")

        for model_result in models_to_save:
            try:
                # Extract model_name and model_group based on format
                if 'model_name' in model_result:  # New format
                    model_name = model_result['model_name']
                    model_group = model_result.get('model_group')
                else:  # Legacy format
                    model_name = model_result.get('model')
                    model_group = None

                # Save trained model to disk (if available)
                model_path = None
                trained_model = model_result.get('trained_model')
                if trained_model is not None and model_result.get('execution_success', True):
                    try:
                        from services.model_persistence import save_model
                        model_path = save_model(
                            model=trained_model,
                            filename=filename,
                            model_name=model_name,
                            timestamp=run_timestamp
                        )
                        print(f"DEBUG: Model saved to {model_path}")
                    except Exception as e:
                        print(f"Warning: Failed to save model {model_name} to disk: {e}")

                print(f"DEBUG: Saving {model_name} to database...")
                result_id = save_model_result(
                    filename=filename,
                    model_name=model_name,
                    model_group=model_group,
                    use_smote=use_smote,
                    selected_features={
                        'base': selected_features,
                        'transformed': result.get('selected_features_transformed', [])
                    },
                    metrics=model_result.get('metrics'),
                    timestamp=run_timestamp,
                    execution_time=model_result.get('execution_time'),
                    execution_success=model_result.get('execution_success', True),
                    error_message=model_result.get('error_message'),
                    model_path=model_path  # NEW: Pass model path to database
                )
                print(f"DEBUG: Saved {model_name} with ID {result_id}")
            except Exception as e:
                print(f"ERROR: Failed to store model result for {model_name}: {str(e)}")
                import traceback
                traceback.print_exc()

        # Persist derived summaries: group-wise bests and benchmark
        try:
            # Persist best model per group
            bests = result.get('best_models_by_group', {}) or {}
            for group_name, info in bests.items():
                try:
                    save_group_best_result(
                        filename=filename,
                        run_timestamp=run_timestamp,
                        model_group=group_name,
                        model_name=info.get('model_name') or info.get('model'),
                        metrics=info.get('metrics') or info
                    )
                except Exception as e:
                    print(f"Warning: Failed to persist group best for {group_name}: {str(e)}")

            # Persist benchmark (best overall)
            bm = result.get('benchmark_model')
            if bm:
                try:
                    save_benchmark_result(
                        filename=filename,
                        run_timestamp=run_timestamp,
                        model_group=bm.get('model_group'),
                        model_name=bm.get('model_name') or bm.get('model'),
                        metrics=bm.get('metrics') or bm
                    )
                except Exception as e:
                    print(f"Warning: Failed to persist benchmark model: {str(e)}")
        except Exception as e:
            print(f"Warning: Error while persisting derived summaries: {str(e)}")

        # Remove trained_model from response (can't serialize sklearn models to JSON)
        # Models are already saved to disk, so we don't need to send them to frontend
        if 'all_models' in result:
            for model_result in result['all_models']:
                model_result.pop('trained_model', None)
        if 'results' in result:
            for model_result in result['results']:
                model_result.pop('trained_model', None)

        # Add timestamp to result so frontend can reference it for statistical tests
        result['timestamp'] = run_timestamp.isoformat()

        return result

    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error running models: {str(e)}")


@app.post("/run-models-stream")
async def run_models_stream_endpoint(request: RunModelsRequest):
    """
    Execute models and stream progress updates using Server-Sent Events (SSE).
    Sends progress updates as each model completes, allowing real-time progress visualization.
    """
    import json
    from fastapi.responses import StreamingResponse

    filename = request.filename
    selected_features = request.selected_features
    target = request.target
    use_smote = request.use_smote

    # Load data from database
    df = load_csv_data(filename)
    if df is None:
        raise HTTPException(status_code=404, detail=f"No CSV data found for file: {filename}")

    # Queue to collect progress events
    progress_events = []

    def progress_callback(event_type: str, **kwargs):
        """Callback invoked during model execution"""
        event_data = {
            'event': event_type,
            **kwargs
        }
        progress_events.append(event_data)

    async def event_generator():
        """Generator that yields SSE formatted events"""
        try:
            # Run models with progress callback
            result = run_models(
                df,
                selected_features,
                target,
                use_smote,
                selected_models=request.selected_models,
                selected_model_groups=request.selected_model_groups,
                progress_callback=progress_callback
            )

            # Send all accumulated progress events
            for event in progress_events:
                yield f"data: {json.dumps(event)}\n\n"

            # After execution, save results to database
            deleted_count = delete_model_results(filename)
            print(f"Deleted {deleted_count} old model result(s) for dataset '{filename}'")

            # Also delete SHAP results since they're based on model results and may be outdated
            shap_deleted_count = delete_shap_result(filename)
            if shap_deleted_count > 0:
                print(f"Deleted {shap_deleted_count} SHAP result(s) for dataset '{filename}' (invalidated by model rerun)")

            # Use a common timestamp for all models in this run
            from datetime import datetime as _dt
            run_timestamp = _dt.now()

            # Store each model result in database
            models_to_save = result.get('all_models', result.get('results', []))

            for model_result in models_to_save:
                try:
                    if 'model_name' in model_result:
                        model_name = model_result['model_name']
                        model_group = model_result.get('model_group')
                    else:
                        model_name = model_result.get('model')
                        model_group = None

                    # Save trained model to disk (if available)
                    model_path = None
                    trained_model = model_result.get('trained_model')
                    if trained_model is not None and model_result.get('execution_success', True):
                        try:
                            from services.model_persistence import save_model
                            model_path = save_model(
                                model=trained_model,
                                filename=filename,
                                model_name=model_name,
                                timestamp=run_timestamp
                            )
                            print(f"DEBUG: Model saved to {model_path}")
                        except Exception as e:
                            print(f"Warning: Failed to save model {model_name} to disk: {e}")

                    result_id = save_model_result(
                        filename=filename,
                        model_name=model_name,
                        model_group=model_group,
                        use_smote=use_smote,
                        selected_features={
                            'base': selected_features,
                            'transformed': result.get('selected_features_transformed', [])
                        },
                        metrics=model_result.get('metrics'),
                        timestamp=run_timestamp,
                        execution_time=model_result.get('execution_time'),
                        execution_success=model_result.get('execution_success', True),
                        error_message=model_result.get('error_message'),
                        model_path=model_path  # NEW: Pass model path to database
                    )
                except Exception as e:
                    print(f"ERROR: Failed to store model result for {model_name}: {str(e)}")

            # Persist derived summaries
            try:
                bests = result.get('best_models_by_group', {}) or {}
                for group_name, info in bests.items():
                    try:
                        save_group_best_result(
                            filename=filename,
                            run_timestamp=run_timestamp,
                            model_group=group_name,
                            model_name=info.get('model_name') or info.get('model'),
                            metrics=info.get('metrics') or info
                        )
                    except Exception as e:
                        print(f"Warning: Failed to persist group best for {group_name}: {str(e)}")

                bm = result.get('benchmark_model')
                if bm:
                    try:
                        save_benchmark_result(
                            filename=filename,
                            run_timestamp=run_timestamp,
                            model_group=bm.get('model_group'),
                            model_name=bm.get('model_name') or bm.get('model'),
                            metrics=bm.get('metrics') or bm
                        )
                    except Exception as e:
                        print(f"Warning: Failed to persist benchmark model: {str(e)}")
            except Exception as e:
                print(f"Warning: Error while persisting derived summaries: {str(e)}")

            # Remove trained_model from response (can't serialize sklearn models to JSON)
            # Models are already saved to disk, so we don't need to send them to frontend
            if 'all_models' in result:
                for model_result in result['all_models']:
                    model_result.pop('trained_model', None)
            if 'results' in result:
                for model_result in result['results']:
                    model_result.pop('trained_model', None)

            # Add timestamp to result so frontend can reference it for statistical tests
            result['timestamp'] = run_timestamp.isoformat()

            # Send final completion event with full results
            final_event = {
                'event': 'execution_complete',
                'result': result,
                'success': True
            }
            yield f"data: {json.dumps(final_event)}\n\n"

        except Exception as e:
            print(f"ERROR in model execution stream: {str(e)}")
            import traceback
            traceback.print_exc()
            # Send error event
            error_event = {
                'event': 'execution_error',
                'error': str(e),
                'success': False
            }
            yield f"data: {json.dumps(error_event)}\n\n"

    # Return streaming response with SSE format
    return StreamingResponse(event_generator(), media_type="text/event-stream")


@app.get("/model-results")
async def get_model_results(filename: str, grouped: bool = True):
    """
    Get all model results for a dataset.

    View mode MUST use this endpoint to display persisted model executions.
    This endpoint returns ONLY what exists in the database - no inference, no defaults.

    Args:
        filename: Dataset filename
        grouped: If True, group results by timestamp (run). If False, return flat list.

    Returns:
        If grouped=True: {"runs": [{"timestamp": ..., "use_smote": ..., "models": [...]}]}
        If grouped=False: {"results": [...]} (flat list)
    """
    try:
        results = load_model_results(filename)

        if not grouped:
            return {"results": results}

        # Group by timestamp
        from collections import defaultdict
        runs_dict = defaultdict(list)

        for result in results:
            timestamp = result['timestamp']
            runs_dict[timestamp].append(result)

        # Convert to structured runs
        runs = []
        for timestamp, models in runs_dict.items():
            # All models in same run share same use_smote and selected_features
            first = models[0]

            # Attempt to load persisted derived summaries for this run
            from datetime import datetime as _dt
            try:
                run_ts_dt = _dt.fromisoformat(timestamp)
            except Exception:
                run_ts_dt = None

            best_by_group = load_group_bests(filename, run_ts_dt) if run_ts_dt else []
            benchmark_result = load_benchmark_for_run(filename, run_ts_dt) if run_ts_dt else None

            run = {
                "run_timestamp": timestamp,
                "use_smote": first['use_smote'],
                "selected_features": first['selected_features'],
                "models": [
                    {
                        "id": m['id'],
                        "model": m['model_name'],
                        "model_group": m.get('model_group'),
                        "metrics": m['metrics']
                    }
                    for m in models
                ],
                "best_by_group": best_by_group,
                "benchmark_result": benchmark_result
            }
            runs.append(run)

        # Sort by timestamp descending (most recent first)
        runs.sort(key=lambda r: r['run_timestamp'], reverse=True)

        return {"runs": runs}

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error loading model results: {str(e)}")


@app.get("/list-model-runs")
async def list_model_runs():
    """
    List all model runs across all datasets.
    """
    try:
        runs = list_all_model_runs()
        return {"runs": runs}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error listing model runs: {str(e)}")


@app.get("/model-run-details")
async def get_model_run_details_endpoint(run_id: int):
    """
    Get detailed information about a specific model run by ID.
    """
    try:
        details = get_model_run_details_by_id(run_id)
        if details is None:
            raise HTTPException(status_code=404, detail=f"Model run {run_id} not found")
        return details
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error loading model run details: {str(e)}")


@app.delete("/delete-model-results")
async def delete_model_results_endpoint(filename: str):
    """
    Delete all model results for a dataset (both database records and model files).
    """
    try:
        # Delete model files from disk
        from services.model_persistence import delete_dataset_models
        models_deleted = delete_dataset_models(filename)

        # Delete database records
        count = delete_model_results(filename)

        return {
            "success": True,
            "message": f"Deleted {count} model results and {models_deleted} model files for dataset '{filename}'"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error deleting model results: {str(e)}")


@app.delete("/delete-model-run")
async def delete_model_run_endpoint(run_id: int):
    """
    Delete a specific model run.
    """
    try:
        deleted = delete_model_run(run_id)
        if not deleted:
            raise HTTPException(status_code=404, detail=f"Model run {run_id} not found")
        return {
            "success": True,
            "message": f"Model run {run_id} deleted successfully"
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error deleting model run: {str(e)}")


# ==================== SHAP ANALYSIS ENDPOINTS ====================

@app.get("/available-models")
async def get_available_models(filename: str):
    """
    Get list of all available trained models for a dataset.
    Used by frontend to populate model selection UI for SHAP analysis.

    Returns:
        List of models grouped by run timestamp with metrics
    """
    try:
        from persistence.model_repository import load_model_results

        results = load_model_results(filename)

        if not results:
            return {
                "success": True,
                "models": [],
                "runs": []
            }

        # Group models by run timestamp
        runs = {}
        for result in results:
            ts = result['timestamp']
            if ts not in runs:
                runs[ts] = {
                    'timestamp': ts,
                    'models': []
                }

            runs[ts]['models'].append({
                'model_name': result['model_name'],
                'model_group': result.get('model_group'),
                'metrics': result.get('metrics', {}),
                'execution_time': result.get('execution_time'),
                'has_model_file': bool(result.get('model_path'))
            })

        # Sort runs by timestamp (newest first)
        sorted_runs = sorted(runs.values(), key=lambda x: x['timestamp'], reverse=True)

        # Flatten all models
        all_models = [
            {
                **model,
                'run_timestamp': run['timestamp']
            }
            for run in sorted_runs
            for model in run['models']
        ]

        return {
            "success": True,
            "runs": sorted_runs,  # Grouped by run
            "models": all_models,  # Flat list
            "total_count": len(all_models)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error fetching available models: {str(e)}")


@app.post("/shap-analysis")
async def run_shap_analysis_endpoint(request: SHAPAnalysisRequest):
    """
    Run SHAP analysis on a dataset with selected features.

    NEW: Supports multi-model analysis via model_selection parameter.

    Computes:
    - Global SHAP values and feature importance
    - Rank stability across multiple trials
    - Model randomization sanity check
    - Feature direction (positive/negative impact)
    - Summary and bar plots
    """
    if not SHAP_AVAILABLE:
        raise HTTPException(
            status_code=503,
            detail="SHAP analysis is unavailable. PyTorch failed to initialize (likely CUDA/GPU issue). "
                   "Please reinstall PyTorch with CPU support or fix your CUDA installation."
        )
    try:
        # Load dataset
        df = load_csv_data(request.filename)
        if df is None:
            raise HTTPException(status_code=404, detail=f"Dataset '{request.filename}' not found")

        # Validate features
        missing = [f for f in request.selected_features if f not in df.columns]
        if missing:
            raise HTTPException(status_code=400, detail=f"Features not found: {missing}")

        # Validate target column
        if request.target not in df.columns:
            raise HTTPException(status_code=400, detail=f"Target column '{request.target}' not found")

        # Prepare kwargs for SHAP analysis
        kwargs = {
            'df': df,
            'selected_features': request.selected_features,
            'filename': request.filename,
            'target_column': request.target,
            'n_trials': request.n_trials,
            'bg_size': request.bg_size,
            'use_smote': request.use_smote
        }

        # Add model_selections if provided
        if request.model_selection and len(request.model_selection) > 0:
            kwargs['model_selections'] = request.model_selection

        # Run SHAP analysis
        result = run_shap_analysis(**kwargs)

        # Save result to database
        save_shap_result(request.filename, result)

        # Save SHAP image paths to image_metadata table for consistency
        if "plots" in result and result["plots"]:
            # Single model mode
            try:
                if result["plots"].get("summary_plot"):
                    save_image_path(request.filename, "shap_summary", result["plots"]["summary_plot"])
                if result["plots"].get("bar_plot"):
                    save_image_path(request.filename, "shap_bar", result["plots"]["bar_plot"])
            except Exception as e:
                print(f"Warning: Failed to save SHAP image paths to image_metadata: {str(e)}")
        elif result.get("multi_model") and "best_model" in result:
            # Multi-model mode - save best model's plots
            try:
                if result["best_model"].get("plots", {}).get("summary_plot"):
                    save_image_path(request.filename, "shap_summary", result["best_model"]["plots"]["summary_plot"])
                if result["best_model"].get("plots", {}).get("bar_plot"):
                    save_image_path(request.filename, "shap_bar", result["best_model"]["plots"]["bar_plot"])
            except Exception as e:
                print(f"Warning: Failed to save SHAP image paths to image_metadata: {str(e)}")

        return {
            "success": True,
            "shap_result": result
        }
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error running SHAP analysis: {str(e)}")


@app.get("/shap-results")
async def get_shap_results(filename: str):
    """
    Get cached SHAP analysis results for a dataset.
    """
    try:
        result = load_shap_result(filename)
        if result is None:
            return {
                "success": True,
                "cached": False,
                "shap_result": None
            }
        return {
            "success": True,
            "cached": True,
            "shap_result": result
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error loading SHAP results: {str(e)}")


@app.delete("/shap-results")
async def delete_shap_results(filename: str):
    """
    Delete SHAP analysis results for a dataset.
    """
    try:
        count = delete_shap_result(filename)
        return {
            "success": True,
            "message": f"Deleted SHAP results for '{filename}'" if count > 0 else f"No SHAP results found for '{filename}'"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error deleting SHAP results: {str(e)}")


# ==================== LOCAL SHAP ANALYSIS ENDPOINTS ====================

@app.post("/local-shap/analyze")
async def analyze_local_shap_endpoint(request: LocalSHAPRequest):
    """
    Analyze a single row using SHAP and generate AI explanation.

    This endpoint:
    1. Loads the best model for the dataset
    2. Computes SHAP values for the specified row
    3. Generates a waterfall plot
    4. Optionally generates AI explanation
    5. Saves results to database
    """
    if not SHAP_AVAILABLE:
        raise HTTPException(
            status_code=503,
            detail="SHAP analysis is unavailable. PyTorch failed to initialize."
        )

    try:
        from services.local_shap_service import analyze_single_row
        from services.llm_service import generate_local_shap_explanation
        from persistence.local_shap_repository import save_local_shap_analysis

        # Run SHAP analysis for single row
        analysis_result = analyze_single_row(
            filename=request.filename,
            row_index=request.row_index,
            run_timestamp=request.run_timestamp,
            n_trials=request.n_trials,
            bg_size=request.bg_size
        )

        # Generate AI explanation if requested
        ai_explanation = None
        print(f"\n>>> Generate AI Explanation requested: {request.generate_ai_explanation}")
        if request.generate_ai_explanation:
            print(">>> Calling generate_local_shap_explanation...")
            ai_explanation = generate_local_shap_explanation(
                row_data=analysis_result['row_data'],
                shap_contributions=analysis_result['shap_contributions'],
                predicted_prob=analysis_result['predicted_prob'],
                actual_target=analysis_result['actual_target'],
                model_info=analysis_result['model_info'],
                reliability=analysis_result['reliability']
            )
            print(f">>> AI Explanation result: {list(ai_explanation.keys()) if ai_explanation else None}")
            analysis_result['ai_explanation'] = ai_explanation
        else:
            print(">>> AI explanation NOT requested (generate_ai_explanation=False)")

        # Save to database
        db_id = save_local_shap_analysis(
            filename=request.filename,
            row_index=request.row_index,
            analysis_data=analysis_result,
            ai_explanation=ai_explanation,
            waterfall_plot_path=analysis_result.get('waterfall_plot'),
            model_name=analysis_result['model_info'].get('model_name'),
            run_timestamp=analysis_result['model_info'].get('run_timestamp')
        )

        analysis_result['db_id'] = db_id

        # Convert numpy types to Python types for JSON serialization
        from persistence.local_shap_repository import convert_numpy_types
        analysis_result_clean = convert_numpy_types(analysis_result)

        return {
            "success": True,
            "analysis": analysis_result_clean
        }

    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error analyzing row: {str(e)}")


@app.get("/local-shap/analyses")
async def list_local_shap_analyses_endpoint(filename: str, limit: int = 50):
    """
    List all local SHAP analyses for a dataset.

    Returns summary information for each analysis.
    """
    try:
        from persistence.local_shap_repository import list_local_shap_analyses

        analyses = list_local_shap_analyses(filename, limit)

        return {
            "success": True,
            "analyses": analyses,
            "count": len(analyses)
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error listing analyses: {str(e)}")


@app.get("/local-shap/analyses/{analysis_id}")
async def get_local_shap_analysis_endpoint(analysis_id: int):
    """
    Get a specific local SHAP analysis by ID.
    """
    try:
        from persistence.local_shap_repository import get_local_shap_analysis_by_id

        analysis = get_local_shap_analysis_by_id(analysis_id)

        if analysis is None:
            raise HTTPException(status_code=404, detail="Analysis not found")

        return {
            "success": True,
            "analysis": analysis
        }

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error retrieving analysis: {str(e)}")


@app.get("/local-shap/row/{filename}/{row_index}")
async def get_local_shap_for_row_endpoint(filename: str, row_index: int):
    """
    Get cached local SHAP analysis for a specific row.
    """
    try:
        from persistence.local_shap_repository import load_local_shap_analysis

        analysis = load_local_shap_analysis(filename, row_index)

        if analysis is None:
            return {
                "success": True,
                "cached": False,
                "analysis": None
            }

        return {
            "success": True,
            "cached": True,
            "analysis": analysis
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error loading analysis: {str(e)}")


@app.delete("/local-shap/analyses")
async def delete_local_shap_analyses_endpoint(filename: str, row_index: int = None):
    """
    Delete local SHAP analyses.

    If row_index is provided, deletes only that row's analysis.
    Otherwise, deletes all analyses for the dataset.
    """
    try:
        from persistence.local_shap_repository import delete_local_shap_analysis

        count = delete_local_shap_analysis(filename, row_index)

        if row_index is not None:
            message = f"Deleted analysis for row {row_index}" if count > 0 else "No analysis found"
        else:
            message = f"Deleted {count} analyses for '{filename}'"

        return {
            "success": True,
            "message": message,
            "deleted_count": count
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error deleting analyses: {str(e)}")


@app.get("/local-shap/llm-status")
async def check_llm_service_status():
    """
    Check if LLM service is available and configured.
    """
    try:
        from services.llm_service import test_llm_service

        available = test_llm_service()

        return {
            "success": True,
            "llm_available": available,
            "message": "OpenAI API is configured and accessible" if available else "OpenAI API key not configured or invalid"
        }

    except Exception as e:
        return {
            "success": True,
            "llm_available": False,
            "message": f"Error checking LLM service: {str(e)}"
        }


@app.post("/label-randomization")
async def run_label_randomization_endpoint(request: LabelRandomizationRequest):
    """
    Generate SHAP values for a model trained on randomized labels.

    This implements the label randomization sanity check from Adebayo et al. (2018).
    Used to verify that explanations change meaningfully when labels are shuffled.
    """
    print(f"\n{'='*80}")
    print(f"LABEL RANDOMIZATION ENDPOINT CALLED")
    print(f"Filename: {request.filename}")
    print(f"Features: {request.selected_features}")
    print(f"Target: {request.target}")
    print(f"SHAP_AVAILABLE: {SHAP_AVAILABLE}")
    print(f"{'='*80}\n")

    if not SHAP_AVAILABLE:
        error_msg = "SHAP analysis is unavailable. PyTorch failed to initialize."
        print(f"ERROR: {error_msg}")
        raise HTTPException(status_code=503, detail=error_msg)

    try:
        print("Step 1: Importing dependencies...")
        from services.label_randomization_service import generate_randomized_shap_values
        from persistence.db import get_engine
        import json
        print("    Dependencies imported")

        # Load dataset
        print(f"Step 2: Loading dataset '{request.filename}'...")
        df = load_csv_data(request.filename)
        if df is None:
            error_msg = f"Dataset '{request.filename}' not found"
            print(f"   ERROR: {error_msg}")
            raise HTTPException(status_code=404, detail=error_msg)
        print(f"    Dataset loaded: shape={df.shape}")

        # Validate features
        print("Step 3: Validating features...")
        missing = [f for f in request.selected_features if f not in df.columns]
        if missing:
            error_msg = f"Features not found: {missing}"
            print(f"   ERROR: {error_msg}")
            raise HTTPException(status_code=400, detail=error_msg)
        print(f"    All features present")

        # Validate target column
        print(f"Step 4: Validating target column '{request.target}'...")
        if request.target not in df.columns:
            error_msg = f"Target column '{request.target}' not found"
            print(f"   ERROR: {error_msg}")
            raise HTTPException(status_code=400, detail=error_msg)
        print(f"    Target column found")

        # Generate randomized SHAP values
        print("Step 5: Generating randomized SHAP values...")
        result = generate_randomized_shap_values(
            df=df,
            selected_features=request.selected_features,
            target_column=request.target,
            n_samples=request.n_samples,
            random_state=request.random_state
        )
        print(f"    SHAP values generated (accuracy: {result['random_accuracy']:.4f})")

        # Convert numpy arrays to lists for JSON serialization
        print("Step 6: Converting to JSON-serializable format...")
        result_serializable = {
            'randomized_shap_values': result['randomized_shap_values'].tolist() if hasattr(result['randomized_shap_values'], 'tolist') else result['randomized_shap_values'],
            'feature_names': result['feature_names'],
            'base_value': float(result['base_value']),
            'random_accuracy': float(result['random_accuracy']),
            'n_test_samples': result['n_test_samples'],
            'model_type': result['model_type']
        }
        print(f"    Converted to serializable format")

        # Store result in database for later use
        print("Step 7: Storing in database...")
        engine = get_engine()
        # Get raw connection from SQLAlchemy engine
        with engine.connect() as conn:
            raw_conn = conn.connection
            cursor = raw_conn.cursor()

            # Create table if it doesn't exist
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS label_randomization_results (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    filename TEXT NOT NULL,
                    result_data TEXT NOT NULL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(filename)
                )
            ''')

            # Insert or replace result
            cursor.execute('''
                INSERT OR REPLACE INTO label_randomization_results (filename, result_data)
                VALUES (?, ?)
            ''', (request.filename, json.dumps(result_serializable)))

            raw_conn.commit()
        print(f"    Stored in database")

        print(f"\n{'='*80}")
        print("LABEL RANDOMIZATION COMPLETED SUCCESSFULLY")
        print(f"{'='*80}\n")

        return {
            "success": True,
            "result": result_serializable,
            "message": "Label randomization test completed successfully"
        }

    except HTTPException:
        raise
    except Exception as e:
        print(f"\n{'='*80}")
        print(f"ERROR in label randomization endpoint:")
        print(f"Error type: {type(e).__name__}")
        print(f"Error message: {str(e)}")
        print(f"{'='*80}\n")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error running label randomization: {str(e)}")


@app.get("/label-randomization-results")
async def get_label_randomization_results(filename: str):
    """Get cached label randomization results for a dataset."""
    try:
        from persistence.db import get_engine
        import json

        engine = get_engine()
        with engine.connect() as conn:
            raw_conn = conn.connection
            cursor = raw_conn.cursor()

            cursor.execute(
                'SELECT result_data FROM label_randomization_results WHERE filename = ?',
                (filename,)
            )
            row = cursor.fetchone()

        if row is None:
            return {
                "success": True,
                "cached": False,
                "result": None
            }

        return {
            "success": True,
            "cached": True,
            "result": json.loads(row[0])
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error loading label randomization results: {str(e)}")


# ==================== STATISTICAL SIGNIFICANCE TESTS ENDPOINTS ====================

from utils.date_utils import parse_run_timestamp

@app.post("/statistical-tests")
async def run_statistical_tests_endpoint(request: StatisticalTestRequest):
    print("STATISTICAL TESTS ENDPOINT CALLED")
    print(f"Request object: {request}")
    print(f"Request dict: {request.dict()}")
    """
    Run pairwise statistical significance tests on trained models.

    Performs three types of tests:
    - Wilcoxon Signed-Rank Test: Compares paired performance metrics (e.g., AUC, F1)
    - McNemar's Test: Compares classification predictions (error patterns)
    - DeLong's Test: Compares ROC AUC values with proper variance estimation

    Args:
        request: Contains filename, run_timestamp, model pairs, test types, features, target

    Returns:
        Dictionary with test results for all requested model pairs and test types
    """
    print("=" * 80)
    print("STATISTICAL TESTS ENDPOINT - POST REQUEST")
    print("=" * 80)
    print(f"Filename: {request.filename}")
    print(f"Run Timestamp: {request.run_timestamp}")
    print(f"Model Pairs: {request.model_pairs}")
    print(f"Test Types: {request.test_types}")
    print(f"Selected Features: {request.selected_features}")
    print(f"Use SMOTE: {request.use_smote}")
    print("=" * 80)
    print("STARTING STATISTICAL TESTS PROCESSING")
    print("=" * 80)

    try:
        from services.statistical_tests_service import (
            wilcoxon_signed_rank_test,
            mcnemar_test,
            delong_test,
            save_statistical_test_result
        )
        from datetime import datetime
        import json
        from sklearn.model_selection import cross_val_predict, StratifiedKFold
        from sklearn.metrics import roc_auc_score
        import numpy as np

        # Load dataset
        df = load_csv_data(request.filename)
        if df is None:
            raise HTTPException(status_code=404, detail=f"Dataset '{request.filename}' not found")

        # Validate features and target
        missing = [f for f in request.selected_features if f not in df.columns]
        if missing:
            raise HTTPException(status_code=400, detail=f"Features not found: {missing}")

        if request.target not in df.columns:
            raise HTTPException(status_code=400, detail=f"Target column '{request.target}' not found")

        # Parse run timestamp using helper
        run_timestamp = parse_run_timestamp(request.run_timestamp)
        if run_timestamp is None:
            raise HTTPException(status_code=400, detail=f"Invalid timestamp format: {request.run_timestamp}")

        # CRITICAL: Load the first model metadata to get the features it was trained with
        # We need to use the SAME features that the models were trained with
        all_model_results = load_model_results(request.filename)
        matching_model = next((m for m in all_model_results
                              if m['model_name'] == request.model_pairs[0]["model_1_name"]
                              and m['timestamp'] == request.run_timestamp), None)

        if matching_model is None:
            raise HTTPException(status_code=404, detail=f"Model '{request.model_pairs[0]['model_1_name']}' not found")

        # Use the features from the model training, not from the request
        actual_features = matching_model['selected_features']
        print("="*80)
        print("CRITICAL FIX APPLIED - Using model's training features")
        print(f"Model: {matching_model['model_name']}")
        print(f"Features ({len(actual_features)}): {actual_features}")
        print("="*80)

        # Prepare data - ensure consistent preprocessing with model training
        X = df[actual_features].copy()
        y = df[request.target].copy()

        # Handle categorical encoding if needed (same as in model training)
        for col in X.select_dtypes(include=['object', 'category']).columns:
            X[col] = X[col].astype(str)

        # Encode target if categorical, and always convert to numpy for consistent indexing
        if y.dtype == 'object' or y.dtype.name == 'category':
            from sklearn.preprocessing import LabelEncoder
            le_target = LabelEncoder()
            y = le_target.fit_transform(y.astype(str))
        else:
            y = y.values

        # Apply SMOTE if requested (same as in model training)
        # NOTE: SMOTE should only be applied to numerical data.
        # If categorical data is present, we skip SMOTE on raw data to avoid errors.
        if request.use_smote:
            from imblearn.over_sampling import SMOTE
            try:
                cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()
                if not cat_cols:
                    smote = SMOTE(random_state=42)
                    X, y = smote.fit_resample(X, y)
                    print(f"Applied SMOTE to raw data: {X.shape[0]} samples")
                else:
                    print(f"DEBUG: Skipping SMOTE on raw data due to categorical columns: {cat_cols}")
            except Exception as e:
                print(f"Warning: SMOTE failed: {e}. Continuing without SMOTE.")

        # Load trained models from persistence
        from services.model_persistence import load_model_by_name

        all_results = []

        print(f"About to process {len(request.model_pairs)} model pairs")

        # Process each model pair
        for pair in request.model_pairs:
            model_1_name = pair["model_1_name"]
            model_2_name = pair["model_2_name"]

            print(f"DEBUG: Processing pair {model_1_name} vs {model_2_name}")

            try:
                # Load models
                print(f"DEBUG: Loading model {model_1_name}")
                model_1 = load_model_by_name(request.filename, model_1_name, run_timestamp)
                print(f"DEBUG: Loading model {model_2_name}")
                model_2 = load_model_by_name(request.filename, model_2_name, run_timestamp)

                if model_1 is None or model_2 is None:
                    print(f"DEBUG: Could not load models {model_1_name} or {model_2_name}. Skipping pair.")
                    # Write to log file for debugging
                    with open('C:/Python/Elucidate/try 7/model_selection_process_react/backend/stat_tests_debug.log', 'a') as f:
                        f.write(f"Model loading failed: {model_1_name}={model_1 is not None}, {model_2_name}={model_2 is not None}\n")
                        f.write(f"Timestamp: {run_timestamp}\n")
                        f.write(f"Filename: {request.filename}\n")
                    continue

                print(f"DEBUG: Successfully loaded both models")

                # Generate predictions for both models
                cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

                # For Wilcoxon: collect AUC scores across folds
                if "wilcoxon" in request.test_types:
                    auc_scores_1 = []
                    auc_scores_2 = []

                    print(f"DEBUG: Starting Wilcoxon CV for {model_1_name} vs {model_2_name}")
                    fold_idx = 0
                    for train_idx, test_idx in cv.split(X, y):
                        fold_idx += 1
                        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
                        y_train, y_test = y[train_idx], y[test_idx]

                        # Retrain models on fold (necessary for proper CV)
                        # We need fresh copies of the models for each fold
                        model_1_fold = load_model_by_name(request.filename, model_1_name, run_timestamp)
                        model_2_fold = load_model_by_name(request.filename, model_2_name, run_timestamp)

                        if model_1_fold is None or model_2_fold is None:
                            print(f"DEBUG: Failed to load model copies for fold {fold_idx}")
                            continue

                        model_1_fold.fit(X_train, y_train)
                        model_2_fold.fit(X_train, y_train)

                        # Get probability predictions
                        if hasattr(model_1_fold, 'predict_proba'):
                            y_score_1 = model_1_fold.predict_proba(X_test)[:, 1]
                            y_score_2 = model_2_fold.predict_proba(X_test)[:, 1]
                        else:
                            y_score_1 = model_1_fold.decision_function(X_test)
                            y_score_2 = model_2_fold.decision_function(X_test)

                        auc_1 = roc_auc_score(y_test, y_score_1)
                        auc_2 = roc_auc_score(y_test, y_score_2)

                        auc_scores_1.append(auc_1)
                        auc_scores_2.append(auc_2)
                    
                    if len(auc_scores_1) >= 5:
                        # Run Wilcoxon test
                        wilcoxon_result = wilcoxon_signed_rank_test(
                            np.array(auc_scores_1),
                            np.array(auc_scores_2),
                            metric_name="AUC"
                        )

                        print(f"DEBUG: Wilcoxon result: {wilcoxon_result}")

                        # Save to database
                        save_statistical_test_result(
                            filename=request.filename,
                            run_timestamp=run_timestamp,
                            model_1_name=model_1_name,
                            model_2_name=model_2_name,
                            test_result=wilcoxon_result
                        )

                        all_results.append({
                            "model_pair": f"{model_1_name} vs {model_2_name}",
                            "test": "wilcoxon",
                            "result": wilcoxon_result
                        })
                    else:
                        print(f"DEBUG: Not enough folds completed for Wilcoxon ({len(auc_scores_1)})")

                # For McNemar and DeLong: use full dataset predictions
                try:
                    print(f"DEBUG: Making predictions with model_1 on {X.shape[0]} samples")
                    y_pred_1 = model_1.predict(X)
                    print(f"DEBUG: Making predictions with model_2")
                    y_pred_2 = model_2.predict(X)

                    if hasattr(model_1, 'predict_proba'):
                        y_score_1 = model_1.predict_proba(X)[:, 1]
                        y_score_2 = model_2.predict_proba(X)[:, 1]
                    else:
                        y_score_1 = model_1.decision_function(X)
                        y_score_2 = model_2.decision_function(X)

                    print(f"DEBUG: Predictions successful - shapes: pred1={y_pred_1.shape}, pred2={y_pred_2.shape}")
                except Exception as e:
                    print(f"ERROR: Failed to make predictions: {str(e)}")
                    import traceback
                    traceback.print_exc()
                    continue

                # McNemar's test
                if "mcnemar" in request.test_types:
                    mcnemar_result = mcnemar_test(
                        y_true=y,
                        y_pred_1=y_pred_1,
                        y_pred_2=y_pred_2
                    )

                    save_statistical_test_result(
                        filename=request.filename,
                        run_timestamp=run_timestamp,
                        model_1_name=model_1_name,
                        model_2_name=model_2_name,
                        test_result=mcnemar_result
                    )

                    all_results.append({
                        "model_pair": f"{model_1_name} vs {model_2_name}",
                        "test": "mcnemar",
                        "result": mcnemar_result
                    })

                # DeLong's test
                if "delong" in request.test_types:
                    delong_result = delong_test(
                        y_true=y,
                        y_score_1=y_score_1,
                        y_score_2=y_score_2
                    )

                    save_statistical_test_result(
                        filename=request.filename,
                        run_timestamp=run_timestamp,
                        model_1_name=model_1_name,
                        model_2_name=model_2_name,
                        test_result=delong_result
                    )

                    all_results.append({
                        "model_pair": f"{model_1_name} vs {model_2_name}",
                        "test": "delong",
                        "result": delong_result
                    })

            except Exception as e:
                print(f"Error testing {model_1_name} vs {model_2_name}: {str(e)}")
                import traceback
                traceback.print_exc()
                # Log to file for detailed debugging
                with open('C:/Python/Elucidate/try 7/model_selection_process_react/backend/stat_tests_error.log', 'a') as f:
                    f.write(f"\n{'='*80}\n")
                    f.write(f"Error in pair: {model_1_name} vs {model_2_name}\n")
                    f.write(f"Error: {str(e)}\n")
                    f.write(f"Traceback:\n")
                    traceback.print_exc(file=f)
                    f.write(f"{'='*80}\n")
                continue

        print("=" * 80)
        print(f"STATISTICAL TESTS COMPLETED: {len(all_results)} results")
        print("Results summary:")
        for i, res in enumerate(all_results[:3]):  # Print first 3
            print(f"  {i+1}. {res['model_pair']} - {res['test']} - p={res['result']['p_value']:.4f}")
        if len(all_results) > 3:
            print(f"  ... and {len(all_results) - 3} more")
        print("=" * 80)

        # Convert numpy types to Python native types for JSON serialization
        def convert_numpy_types(obj):
            """Recursively convert numpy types to Python native types"""
            import numpy as np
            if isinstance(obj, dict):
                return {k: convert_numpy_types(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy_types(item) for item in obj]
            elif isinstance(obj, (np.bool_, np.integer, np.floating)):
                return obj.item()
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            else:
                return obj

        # Convert all_results to remove numpy types
        serializable_results = convert_numpy_types(all_results)

        response = {
            "success": True,
            "message": f"Completed {len(all_results)} statistical tests",
            "count": len(all_results),
            "results": serializable_results
        }

        print("Returning response:")
        print(f"  success: {response['success']}")
        print(f"  count: {response['count']}")
        print(f"  results length: {len(response['results'])}")
        print("=" * 80)

        return response

    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error running statistical tests: {str(e)}")


@app.get("/statistical-tests")
async def get_statistical_test_results(filename: str, run_timestamp: str = None):
    """
    Retrieve statistical test results from database.

    Args:
        filename: Dataset filename
        run_timestamp: Optional ISO format timestamp to filter by specific run

    Returns:
        List of all statistical test results for the dataset
    """
    try:
        from services.statistical_tests_service import get_statistical_test_results
        from datetime import datetime

        run_ts = parse_run_timestamp(run_timestamp) if run_timestamp else None
        results = get_statistical_test_results(filename, run_ts)

        return {
            "success": True,
            "count": len(results),
            "message": f"Retrieved {len(results)} statistical test results",
            "results": results
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error retrieving statistical test results: {str(e)}")


@app.delete("/statistical-tests")
async def delete_statistical_test_results(filename: str, run_timestamp: str = None):
    """
    Delete statistical test results from database.

    Args:
        filename: Dataset filename
        run_timestamp: Optional ISO format timestamp to filter by specific run

    Returns:
        Success message with count of deleted records
    """
    try:
        from services.statistical_tests_service import delete_statistical_test_results
        from datetime import datetime

        run_ts = parse_run_timestamp(run_timestamp) if run_timestamp else None
        count = delete_statistical_test_results(filename, run_ts)

        return {
            "success": True,
            "message": f"Deleted {count} statistical test results for '{filename}'"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error deleting statistical test results: {str(e)}")


# ==================== VALIDATED EXPLANATION ENDPOINT ====================

@app.post("/generate-validated-explanation")
async def generate_validated_explanation_endpoint(request: ValidatedExplanationRequest):
    """
    Generate validated explanation narrative using the falsifiability pipeline.

    This endpoint orchestrates the complete three-layer validation framework:
    1. Layer 1 (Statistical Reliability): Retrieves statistical test results from database
    2. Layer 2 (Explanatory Validity): Validates SHAP explanations through falsification tests
    3. Layer 3 (GenAI Narrative): Generates audit-ready narrative ONLY if validation passed

    Args:
        request: Contains filename, model_name, run_timestamp, use_shap_cache

    Returns:
        Dictionary with:
        - validation_report: Complete validation results with pass/fail for each test
        - narrative: Audit-ready narrative (None if validation failed)
        - success: Boolean indicating whether narrative was generated
        - message: Human-readable status message
    """
    try:
        from services.falsifiability_pipeline import FalsifiabilityPipeline
        from services.model_persistence import load_model_by_name
        from services.statistical_tests_service import get_statistical_test_results
        from persistence.shap_repository import load_shap_result
        import numpy as np

        # Parse timestamp
        run_timestamp = parse_run_timestamp(request.run_timestamp)
        if run_timestamp is None:
            raise HTTPException(status_code=400, detail=f"Invalid timestamp format: {request.run_timestamp}")

        # Load dataset
        df = load_csv_data(request.filename)
        if df is None:
            raise HTTPException(status_code=404, detail=f"Dataset '{request.filename}' not found")

        # Load SHAP results (either cached or fresh)
        shap_result = None
        if request.use_shap_cache:
            shap_result = load_shap_result(request.filename)
            if shap_result is None:
                raise HTTPException(
                    status_code=404,
                    detail=f"No cached SHAP results found for '{request.filename}'. Run SHAP analysis first or set use_shap_cache=false."
                )

        if shap_result is None:
            # Need to run SHAP analysis
            # Load model metadata to get features
            all_model_results = load_model_results(request.filename)
            matching_model = next((m for m in all_model_results
                                  if m['model_name'] == request.model_name
                                  and m['timestamp'] == request.run_timestamp), None)

            if matching_model is None:
                raise HTTPException(status_code=404, detail=f"Model '{request.model_name}' not found for this run")

            selected_features = matching_model['selected_features']
            use_smote = matching_model['use_smote']

            # Run SHAP analysis
            if not SHAP_AVAILABLE:
                raise HTTPException(
                    status_code=503,
                    detail="SHAP analysis is unavailable. PyTorch failed to initialize."
                )

            shap_result = run_shap_analysis(
                df=df,
                selected_features=selected_features,
                filename=request.filename,
                target_column='target',
                n_trials=5,
                bg_size=50,
                use_smote=use_smote
            )

            # Save for future use
            save_shap_result(request.filename, shap_result)

        # Extract SHAP values from global_importance structure
        # Convert aggregated global importance back to array format for validation
        global_importance = shap_result.get('global_importance', [])

        if not global_importance:
            raise HTTPException(
                status_code=400,
                detail="SHAP results do not contain global_importance data"
            )

        # Sort by rank and extract feature names and mean SHAP values
        sorted_importance = sorted(global_importance, key=lambda x: x['rank'])
        feature_names = [item['feature'] for item in sorted_importance]
        shap_values = np.array([item['mean_shap'] for item in sorted_importance])

        # Use model metadata to get base value (if available) or estimate
        model_info = shap_result.get('model_info', {})
        base_value = model_info.get('baseline_pred', 0.5)  # Default to 0.5 for binary classification

        # Calculate prediction from base + sum of SHAP values
        prediction = base_value + np.sum(shap_values)

        # Load or generate randomized SHAP values for label randomization test
        randomized_shap_values = None
        label_rand_metrics = None
        try:
            from persistence.db import get_engine
            import json

            # First, try to load cached randomized SHAP values
            engine = get_engine()
            with engine.connect() as conn:
                raw_conn = conn.connection
                cursor = raw_conn.cursor()
                cursor.execute(
                    'SELECT result_data FROM label_randomization_results WHERE filename = ?',
                    (request.filename,)
                )
                row = cursor.fetchone()

            if row is not None:
                # Use cached randomized values
                rand_result = json.loads(row[0])
                randomized_shap_values = np.array(rand_result['randomized_shap_values'])

                # Compute similarity metrics between original and randomized SHAP
                from services.label_randomization_service import compute_similarity_metrics
                similarity = compute_similarity_metrics(shap_values, randomized_shap_values)

                label_rand_metrics = {
                    'pearson_correlation': similarity['pearson_correlation'],
                    'random_accuracy': rand_result['random_accuracy']
                }

                print(f"Using cached randomized SHAP values (accuracy on random labels: {rand_result['random_accuracy']:.4f})")
            else:
                # Generate randomized SHAP on-the-fly (fast, ~2-3 seconds)
                from services.label_randomization_service import generate_randomized_shap_values, compute_similarity_metrics

                print(f"No cached randomization data found. Attempting to generate on-the-fly...")

                # Get model metadata to retrieve selected features
                all_model_results = load_model_results(request.filename)
                matching_model = next((m for m in all_model_results
                                      if m['model_name'] == request.model_name
                                      and m['timestamp'] == request.run_timestamp), None)

                if matching_model:
                    selected_features = matching_model['selected_features']

                    rand_result = generate_randomized_shap_values(
                        df=df,
                        selected_features=selected_features,
                        target_column='target',
                        n_samples=50,
                        random_state=42
                    )
                    randomized_shap_values = rand_result['randomized_shap_values']

                    # Compute similarity metrics
                    similarity = compute_similarity_metrics(shap_values, randomized_shap_values)

                    label_rand_metrics = {
                        'pearson_correlation': similarity['pearson_correlation'],
                        'random_accuracy': rand_result['random_accuracy']
                    }

                    print(f"Generated randomized SHAP values on-the-fly (accuracy on random labels: {rand_result['random_accuracy']:.4f})")
                    print(f"Label randomization metrics: correlation={similarity['pearson_correlation']:.4f}")
                else:
                    print(f"Warning: Could not find matching model for {request.model_name} at {request.run_timestamp}")
                    print(f"Available models: {[(m['model_name'], m['timestamp']) for m in all_model_results[:5]]}")
        except Exception as e:
            print(f"Warning: Could not load/generate randomized SHAP values: {e}")
            import traceback
            traceback.print_exc()
            # Continue without randomized values - label_rand_metrics stays None

        # Initialize evidence-based pipeline
        from services.falsifiability_pipeline_v2 import EvidenceBasedPipeline
        pipeline = EvidenceBasedPipeline()

        # Dataset info
        dataset_info = {
            'filename': request.filename,
            'description': request.filename.replace('.csv', '').replace('_', ' ').title()
        }

        # Prepare label randomization data if available
        # Pass the computed metrics (pearson_correlation, random_accuracy)
        label_randomization_data = label_rand_metrics

        # Process explanation through evidence-based pipeline
        # This collects all evidence transparently and generates contextualized narrative
        evidence_package = pipeline.process_model_explanation(
            model_name=request.model_name,
            dataset_filename=request.filename,
            run_timestamp=request.run_timestamp,
            shap_values=shap_values,
            feature_names=feature_names,
            base_value=base_value,
            prediction=prediction,
            dataset_info=dataset_info,
            label_randomization_data=label_randomization_data
        )

        # Format response with transparent evidence presentation
        response = {
            "success": True,  # Always succeeds - we collect evidence, not pass/fail
            "evidence_summary": evidence_package.get('evidence_summary', ''),
            "statistical_evidence": evidence_package.get('statistical_evidence', {}),
            "explanatory_evidence": evidence_package.get('explanatory_evidence', {}),
            "ai_narrative": evidence_package.get('ai_narrative', {}),
            "message": "Evidence-based commentary generated successfully. All evidence layers collected and saved to database."
        }

        # Add observational metrics (not pass/fail verdicts)
        statistical_obs = evidence_package.get('statistical_evidence', {})
        if statistical_obs:
            response["statistical_observations"] = {
                "tests_conducted": len(statistical_obs.get('tests_conducted', [])),
                "significant_differences_found": statistical_obs.get('significant_differences_found', False),
                "observations": statistical_obs.get('observations', [])
            }

        explanatory_obs = evidence_package.get('explanatory_evidence', {})
        if explanatory_obs:
            response["explanatory_observations"] = {
                "label_randomization": explanatory_obs.get('label_randomization', {}),
                "rank_stability": explanatory_obs.get('rank_stability', {}),
                "sanity_checks": explanatory_obs.get('sanity_checks', {}),
                "observations": explanatory_obs.get('observations', [])
            }

        # Include AI narrative with metadata
        ai_narrative = evidence_package.get('ai_narrative', {})
        if ai_narrative:
            response["narrative"] = {
                "text": ai_narrative.get('narrative', ''),
                "caveats": ai_narrative.get('caveats', []),
                "confidence_qualifiers": ai_narrative.get('confidence_qualifiers', []),
                "generation_metadata": ai_narrative.get('generation_metadata', {})
            }

        return response

    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error generating validated explanation: {str(e)}")


# ==================== MODEL REGISTRY ENDPOINTS ====================

@app.post("/model-registry/sync")
async def sync_model_registry():
    """
    Sync the MODEL_REGISTRY to database.
    Updates the model_registry_catalog table with current model definitions.
    """
    try:
        from services.model_registry_db import sync_registry_to_database
        result = sync_registry_to_database()
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error syncing model registry: {str(e)}")


@app.get("/model-registry/summary")
async def get_model_registry_summary():
    """
    Get summary of all model groups and their variant counts.
    """
    try:
        from services.model_registry_db import get_model_registry_summary
        summary = get_model_registry_summary()
        return summary
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error fetching model registry summary: {str(e)}")


@app.get("/model-registry/groups")
async def get_all_model_groups():
    """
    Get list of all model group names.
    """
    try:
        from services.model_registry_db import get_all_model_groups
        groups = get_all_model_groups()
        return {"groups": groups}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error fetching model groups: {str(e)}")


@app.get("/model-registry/group/{group_name}")
async def get_models_by_group(group_name: str):
    """
    Get all model variants for a specific group with their details.
    """
    try:
        from services.model_registry_db import get_models_by_group
        models = get_models_by_group(group_name)
        if not models:
            raise HTTPException(status_code=404, detail=f"Model group '{group_name}' not found")
        return {
            "group_name": group_name,
            "models": models
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error fetching models for group '{group_name}': {str(e)}")


@app.get("/historical-runs")
async def get_historical_runs():
    """
    Get list of all historical model runs from the database.
    Returns unique combinations of filename and timestamp.
    """
    try:
        runs = list_all_model_runs()
        return {
            "success": True,
            "runs": runs
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error fetching historical runs: {str(e)}")


@app.post("/reports/generate-individual")
async def generate_individual_reports(request: dict):
    """
    Generate individual LaTeX/PDF reports for selected runs.

    Request body:
    {
        "run_ids": [{"filename": "data.csv", "timestamp": "2024-01-01T12:00:00"}]
    }
    """
    from services.latex_service import generate_comprehensive_latex_report, compile_latex_to_pdf
    from services.report_artifacts_collector import collect_all_artifacts
    from persistence.db import get_db

    try:
        run_ids = request.get("run_ids", [])
        if not run_ids:
            raise HTTPException(status_code=400, detail="No run_ids provided")

        reports_generated = []

        for run_id in run_ids:
            filename = run_id.get("filename")
            timestamp = run_id.get("timestamp")

            if not filename or not timestamp:
                continue

            # Collect all artifacts from database
            artifacts = collect_all_artifacts(filename, timestamp)
            if not artifacts.get('run_data'):
                continue

            run_data = artifacts['run_data']

            # Add identifier for file naming
            run_data['run_id'] = f"{filename}_{timestamp}".replace(":", "-").replace(" ", "_")

            # Generate comprehensive LaTeX with Results section
            latex_content = generate_comprehensive_latex_report(**artifacts)

            # Compile to PDF
            output_dir = get_reports_dir()
            output_name = f"report_{run_data['run_id']}"
            compilation_result = compile_latex_to_pdf(latex_content, output_dir, output_name)
            
            # Save to database with comprehensive error info
            db = next(get_db())
            try:
                report = save_report(
                    db=db,
                    report_type="individual",
                    source_run_ids=[f"{filename}|{timestamp}"],
                    tex_path=compilation_result['tex_path'],
                    pdf_path=compilation_result.get('pdf_path'),
                    metadata={
                        "filename": filename,
                        "timestamp": timestamp,
                        "compilation_success": compilation_result['success'],
                        "error_type": compilation_result.get('error_type'),
                        "error_summary": compilation_result.get('error_summary'),
                        "compiler_log_path": compilation_result.get('compiler_log_path'),
                        "recoverable": compilation_result.get('recoverable', False)
                    }
                )

                reports_generated.append(report_to_dict(report))
            finally:
                db.close()
        
        return {
            "success": True,
            "reports": reports_generated
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error generating reports: {str(e)}")


def _generate_combined_comprehensive_latex(all_artifacts: list) -> str:
    """
    Generate combined LaTeX report with comprehensive Results sections for each run.

    Uses the same artifact collection and LaTeX generation approach as individual reports,
    ensuring consistency across both report types.

    Args:
        all_artifacts: List of artifact dicts from collect_combined_artifacts()
                      Each dict contains: run_data, dataset_metadata, pandas_results,
                      fi_results, shap_results, image_paths, smote_results, dataset_quality_summary

    Returns:
        Complete LaTeX document content with document wrapper
    """
    from services.results_section_generator import generate_results_section
    from services.latex_service import _escape_latex, generate_evaluation_metrics_table, generate_model_registry_table
    from datetime import datetime

    latex = []

    # Document header with comprehensive packages
    latex.append("\\documentclass[12pt,a4paper]{article}")
    latex.append("\\usepackage{booktabs}")
    latex.append("\\usepackage{graphicx}")
    latex.append("\\usepackage{float}")
    latex.append("\\usepackage[margin=1in]{geometry}")
    latex.append("\\usepackage{tabularx}")
    latex.append("\\usepackage{lscape}")
    latex.append("\\usepackage{adjustbox}")
    latex.append("\\usepackage{array}")  # For advanced table column formatting
    latex.append("\\usepackage{pgfplots}")  # For performance trend charts
    latex.append("\\usepackage{pgfplotstable}")  # For pgfplots data tables
    latex.append("\\pgfplotsset{compat=1.18}")  # Set pgfplots compatibility
    latex.append("")
    latex.append("\\title{Combined Model Execution Report}")
    latex.append(f"\\author{{Generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}}}")
    latex.append("\\date{}")
    latex.append("")
    latex.append("\\begin{document}")
    latex.append("\\maketitle")
    latex.append("")

    # Add static reference tables (same as individual reports)
    latex.append(generate_evaluation_metrics_table())
    latex.append("")

    latex.append(generate_model_registry_table())
    latex.append("")

    # Overview section
    latex.append("\\section{Overview}")
    latex.append(f"This report consolidates results from {len(all_artifacts)} experimental runs, ")
    latex.append("each analyzed using the same comprehensive evaluation framework.")
    latex.append("")

    # Generate section for each run with full Results content
    for idx, artifacts in enumerate(all_artifacts, 1):
        run_data = artifacts.get('run_data', {})
        filename = run_data.get('filename', f'Run {idx}')

        latex.append(f"\\section{{Run {idx}: {_escape_latex(filename)}}}")
        latex.append("")

        # Metadata subsection
        latex.append("\\subsection{Experiment Configuration}")
        latex.append("\\begin{itemize}")
        latex.append(f"\\item \\textbf{{Dataset:}} {_escape_latex(filename)}")
        latex.append(f"\\item \\textbf{{Execution Timestamp:}} {run_data.get('timestamp', 'Unknown')}")
        latex.append(f"\\item \\textbf{{SMOTE Applied:}} {'Yes' if run_data.get('use_smote', False) else 'No'}")

        features = run_data.get('selected_features', [])
        if features:
            latex.append(f"\\item \\textbf{{Features Selected:}} {len(features)}")
            # Show first 10 features
            if isinstance(features, list) and len(features) > 0:
                feature_preview = [_escape_latex(str(f)) for f in features[:10]]
                latex.append(f"\\item \\textbf{{Feature Preview:}} {', '.join(feature_preview)}")
                if len(features) > 10:
                    latex.append(f"\\item ... and {len(features) - 10} more features")

        latex.append("\\end{itemize}")
        latex.append("")

        # Comprehensive Results section using the same generator as individual reports
        results_section = generate_results_section(
            run_data=run_data,
            dataset_metadata=artifacts.get('dataset_metadata', {}),
            pandas_results=artifacts.get('pandas_results', {}),
            fi_results=artifacts.get('fi_results', {}),
            shap_results=artifacts.get('shap_results', {}),
            image_paths=artifacts.get('image_paths', {}),
            smote_results=artifacts.get('smote_results', {}),
            dataset_quality_summary=artifacts.get('dataset_quality_summary', [])
        )

        if results_section:
            latex.append("\\subsection{Results}")
            latex.append(results_section)
            latex.append("")

    latex.append("\\end{document}")

    return "\n".join(latex)


@app.post("/reports/generate-combined")
async def generate_combined_report(request: dict):
    """
    Generate a single combined LaTeX/PDF report for selected runs with comprehensive Results sections.

    Request body:
    {
        "run_ids": [{"filename": "data1.csv", "timestamp": "2024-01-01T12:00:00"}, ...]
    }
    """
    from services.latex_service import compile_latex_to_pdf
    from services.report_artifacts_collector import collect_combined_artifacts
    from persistence.db import get_db

    try:
        run_ids = request.get("run_ids", [])
        if not run_ids:
            raise HTTPException(status_code=400, detail="No run_ids provided")

        # Collect all artifacts for each run
        all_artifacts = collect_combined_artifacts(run_ids)
        if not all_artifacts:
            raise HTTPException(status_code=400, detail="No valid runs found")

        runs_data = [a.get('run_data') for a in all_artifacts if a.get('run_data')]
        if not runs_data:
            raise HTTPException(status_code=400, detail="No valid model data found")

        # Build source_ids from run_ids
        source_ids = [f"{rid.get('filename')}|{rid.get('timestamp')}" for rid in run_ids]

        # Generate combined LaTeX with Results sections for each run
        latex_content = _generate_combined_comprehensive_latex(all_artifacts)

        # Compile to PDF
        from datetime import datetime
        output_dir = get_reports_dir()
        output_name = f"combined_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        compilation_result = compile_latex_to_pdf(latex_content, output_dir, output_name)

        # Save to database with comprehensive error info
        db = next(get_db())
        try:
            report = save_report(
                db=db,
                report_type="combined",
                source_run_ids=source_ids,
                tex_path=compilation_result['tex_path'],
                pdf_path=compilation_result.get('pdf_path'),
                metadata={
                    "num_runs": len(runs_data),
                    "filenames": [r['filename'] for r in runs_data],
                    "compilation_success": compilation_result['success'],
                    "error_type": compilation_result.get('error_type'),
                    "error_summary": compilation_result.get('error_summary'),
                    "compiler_log_path": compilation_result.get('compiler_log_path'),
                    "recoverable": compilation_result.get('recoverable', False)
                }
            )

            return {
                "success": True,
                "report": report_to_dict(report)
            }
        finally:
            db.close()
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error generating combined report: {str(e)}")


@app.get("/reports/list")
async def list_reports():
    """
    List all generated reports.
    """
    from persistence.db import get_db

    try:
        db = next(get_db())
        try:
            reports = list_all_reports(db)

            return {
                "success": True,
                "reports": [report_to_dict(r) for r in reports]
            }
        finally:
            db.close()

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error listing reports: {str(e)}")


@app.get("/reports/{report_id}/latex")
async def get_report_latex(report_id: int):
    """
    Get LaTeX source content for a report.
    """
    from persistence.db import get_db

    try:
        db = next(get_db())
        try:
            report = get_report_by_id(db, report_id)

            if not report:
                raise HTTPException(status_code=404, detail="Report not found")

            if not report.tex_path or not os.path.exists(report.tex_path):
                raise HTTPException(status_code=404, detail="LaTeX file not found")

            with open(report.tex_path, 'r', encoding='utf-8') as f:
                latex_content = f.read()

            return {
                "success": True,
                "content": latex_content
            }
        finally:
            db.close()

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error reading LaTeX file: {str(e)}")


@app.get("/reports/{report_id}/compiler-log")
async def get_compiler_log(report_id: int):
    """
    Get full compiler log for a report.
    """
    from persistence.db import get_db
    import json

    try:
        db = next(get_db())
        try:
            report = get_report_by_id(db, report_id)

            if not report:
                raise HTTPException(status_code=404, detail="Report not found")

            # Extract compiler log path from metadata
            metadata = json.loads(report.report_metadata) if report.report_metadata else {}
            compiler_log_path = metadata.get('compiler_log_path')

            if not compiler_log_path or not os.path.exists(compiler_log_path):
                raise HTTPException(status_code=404, detail="Compiler log not found")

            with open(compiler_log_path, 'r', encoding='utf-8', errors='ignore') as f:
                log_content = f.read()

            return {
                "success": True,
                "content": log_content,
                "error_type": metadata.get('error_type'),
                "error_summary": metadata.get('error_summary'),
                "recoverable": metadata.get('recoverable', False)
            }
        finally:
            db.close()

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error reading compiler log: {str(e)}")


from fastapi.responses import FileResponse

@app.get("/reports/{report_id}/pdf")
async def get_report_pdf(report_id: int):
    """
    Get PDF file for a report. Serves inline for preview, not as download.
    """
    from persistence.db import get_db

    try:
        db = next(get_db())
        try:
            report = get_report_by_id(db, report_id)

            if not report:
                raise HTTPException(status_code=404, detail="Report not found")

            if not report.pdf_path or not os.path.exists(report.pdf_path):
                raise HTTPException(status_code=404, detail="PDF file not found or compilation failed")

            return FileResponse(
                path=report.pdf_path,
                media_type="application/pdf"
            )
        finally:
            db.close()

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error serving PDF file: {str(e)}")


@app.delete("/reports/{report_id}")
async def delete_report_endpoint(report_id: int, db: Session = Depends(get_db)):
    """
    Delete a report and its associated files.
    """
    try:
        success = delete_report(db, report_id)

        if not success:
            raise HTTPException(status_code=404, detail="Report not found")

        return {
            "success": True,
            "message": f"Report {report_id} deleted successfully"
        }

    except HTTPException:
        raise
    except Exception as e:
        # Log the actual error for debugging
        import traceback
        print(f"Error deleting report {report_id}: {str(e)}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error deleting report: {str(e)}")


# ==================== BULK EXECUTION ENDPOINTS ====================

@app.post("/bulk-execution/start")
async def start_bulk_execution(request: dict):
    """
    Start bulk execution across multiple datasets.

    Request body:
    {
        "run_name": "Experiment 2024-01-15",
        "dataset_filenames": ["data1.csv", "data2.csv", "data3.csv"],
        "execution_config": {
            "selected_model_groups": ["Logistic Regression", "AdaBoost"],
            "use_smote": true,
            "shap_params": {"n_trials": 5, "bg_size": 50}
        }
    }
    """
    try:
        run_name = request.get('run_name')
        dataset_filenames = request.get('dataset_filenames', [])
        execution_config = request.get('execution_config', {})

        if not run_name or not dataset_filenames:
            raise HTTPException(status_code=400, detail="run_name and dataset_filenames required")

        # Validate datasets exist
        for filename in dataset_filenames:
            if not check_dataset_exists(filename):
                raise HTTPException(status_code=404, detail=f"Dataset not found: {filename}")

        # Import bulk execution modules
        from bulk_execution.orchestrator import BulkExecutionOrchestrator

        # Create orchestrator
        db = get_db_session()
        try:
            orchestrator = BulkExecutionOrchestrator(db)

            # Start execution in background (async)
            import asyncio
            bulk_run_id = await orchestrator.execute_bulk_run(
                run_name=run_name,
                dataset_filenames=dataset_filenames,
                execution_config=execution_config
            )

            return {
                'success': True,
                'bulk_run_id': bulk_run_id,
                'message': f'Bulk execution started with {len(dataset_filenames)} datasets'
            }
        finally:
            db.close()

    except HTTPException:
        raise
    except Exception as e:
        import traceback
        logger.error(f"Error starting bulk execution: {str(e)}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error starting bulk execution: {str(e)}")


@app.get("/bulk-execution/status/{bulk_run_id}")
async def get_bulk_execution_status(bulk_run_id: int):
    """Get current status of bulk execution run"""
    try:
        from bulk_execution.progress_tracker import ProgressTracker

        db = get_db_session()
        try:
            tracker = ProgressTracker(db)
            status = tracker.get_run_status(bulk_run_id)

            if not status:
                raise HTTPException(status_code=404, detail="Bulk run not found")

            return {
                'success': True,
                **status
            }
        finally:
            db.close()
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error fetching status: {str(e)}")


@app.get("/bulk-execution/stream/{bulk_run_id}")
async def stream_bulk_execution_progress(bulk_run_id: int):
    """Stream real-time progress updates via SSE"""
    from fastapi.responses import StreamingResponse
    from bulk_execution.progress_tracker import ProgressTracker

    db = get_db_session()
    try:
        tracker = ProgressTracker(db)
        return StreamingResponse(
            tracker.stream_progress(bulk_run_id),
            media_type="text/event-stream"
        )
    except Exception as e:
        logger.error(f"Error streaming bulk execution progress: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error streaming progress: {str(e)}")


@app.get("/bulk-execution/list")
async def list_bulk_executions():
    """List all bulk execution runs"""
    try:
        from bulk_execution.repository import get_all_bulk_runs

        runs = get_all_bulk_runs()
        return {
            'success': True,
            'runs': runs
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error listing bulk runs: {str(e)}")


@app.get("/bulk-execution/details/{bulk_run_id}")
async def get_bulk_execution_details(bulk_run_id: int):
    """Get detailed results of completed bulk run"""
    try:
        from bulk_execution.repository import get_bulk_run_details

        details = get_bulk_run_details(bulk_run_id)
        if not details:
            raise HTTPException(status_code=404, detail="Bulk run not found")

        return {
            'success': True,
            **details
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error fetching details: {str(e)}")


@app.delete("/bulk-execution/{bulk_run_id}")
async def delete_bulk_execution(bulk_run_id: int):
    """Delete bulk execution run (metadata only, not underlying results)"""
    try:
        from bulk_execution.repository import delete_bulk_run

        deleted = delete_bulk_run(bulk_run_id)
        if not deleted:
            raise HTTPException(status_code=404, detail="Bulk run not found")

        return {
            'success': True,
            'message': f'Bulk run {bulk_run_id} deleted'
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error deleting bulk run: {str(e)}")


# ==================== SETTINGS ENDPOINTS ====================

@app.get("/settings")
async def get_settings():
    """
    Get current application settings.
    Returns complete settings structure including paths and metadata.
    """
    try:
        settings_mgr = SettingsManager.get_instance()
        settings = settings_mgr.load_settings()
        return {
            "success": True,
            "settings": settings
        }
    except Exception as e:
        logger = logging.getLogger(__name__)
        logger.exception(f"Error loading settings: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error loading settings: {str(e)}")


@app.put("/settings")
async def update_settings(request: dict):
    """
    Update application settings.
    Validates paths before applying changes.

    Request body:
    {
        "paths": {
            "database_path": "sqlite:///./data/my_database.db",
            "images_dir": "storage/images",
            "reports_dir": "storage/reports",
            "logs_dir": "storage/logs"
        }
    }

    Returns updated settings or validation errors.
    """
    try:
        settings_mgr = SettingsManager.get_instance()
        paths = request.get("paths", {})

        if not paths:
            raise HTTPException(status_code=400, detail="No paths provided")

        # Validate and update
        result = settings_mgr.update_paths(paths)

        if not result.get("success"):
            return {
                "success": False,
                "errors": result.get("errors", {}),
                "message": result.get("message", "Settings validation failed")
            }

        # Reload settings
        updated_settings = settings_mgr.load_settings()

        return {
            "success": True,
            "message": "Settings updated successfully",
            "settings": updated_settings
        }

    except HTTPException:
        raise
    except Exception as e:
        logger = logging.getLogger(__name__)
        logger.exception(f"Error updating settings: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error updating settings: {str(e)}")


@app.post("/settings/reset")
async def reset_settings():
    """
    Reset settings to defaults.
    WARNING: This will reset all path configurations.
    """
    try:
        settings_mgr = SettingsManager.get_instance()
        default_settings = settings_mgr.reset_to_defaults()
        return {
            "success": True,
            "message": "Settings reset to defaults",
            "settings": default_settings
        }
    except Exception as e:
        logger = logging.getLogger(__name__)
        logger.exception(f"Error resetting settings: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error resetting settings: {str(e)}")


@app.get("/settings/validate-path")
async def validate_path(path: str, path_type: str):
    """
    Validate a single path before saving.

    Query params:
        path: Path to validate
        path_type: 'database' or 'directory'

    Returns validation result with valid flag and message.
    """
    try:
        if path_type not in ['database', 'directory']:
            raise HTTPException(status_code=400, detail="path_type must be 'database' or 'directory'")

        settings_mgr = SettingsManager.get_instance()
        validation_result = settings_mgr.validate_single_path(path, path_type)

        return {
            "success": True,
            "valid": validation_result["valid"],
            "message": validation_result.get("message", "")
        }
    except HTTPException:
        raise
    except Exception as e:
        logger = logging.getLogger(__name__)
        logger.exception(f"Error validating path: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error validating path: {str(e)}")


@app.get("/settings/browse")
async def browse_filesystem(path: str = "auto", browse_type: str = "directory"):
    """
    Browse filesystem for directories or database files.

    Query params:
        path: Starting path to browse (default: "auto" to detect best path)
        browse_type: 'directory' or 'database' (default: "directory")

    Returns list of directories/files with their paths.
    """
    try:
        import os
        from pathlib import Path

        # Auto-detect best starting path
        if path == "auto" or path == "/app/data":
            # Try multiple locations in order
            possible_paths = [
                '/app/data',           # Docker mount point
                '/app/backend',        # Docker backend dir
                os.getcwd(),           # Current working directory
                str(Path.home())       # User home directory
            ]

            for test_path in possible_paths:
                if os.path.exists(test_path) and os.access(test_path, os.R_OK):
                    path = test_path
                    break
            else:
                # If none exist, use current directory
                path = '.'

        # Normalize the path
        try:
            browse_path = Path(path).resolve()
        except Exception:
            # If path resolution fails, fall back to current directory
            browse_path = Path.cwd()

        # Security check: prevent browsing outside allowed areas
        # For Docker, allow /app/data and common mount points
        allowed_prefixes = ['/app', '/data', '/mnt', '/opt', '/home', '/tmp']

        # On Windows, allow C:/ and other drives
        if os.name == 'nt':
            allowed_prefixes.extend(['C:', 'D:', 'E:', 'F:', 'C:\\', 'D:\\', 'E:\\', 'F:\\'])

        path_str = str(browse_path)
        is_allowed = any(path_str.startswith(prefix) for prefix in allowed_prefixes)

        if not is_allowed:
            # Try current working directory as fallback
            browse_path = Path.cwd()

        # Check if path exists
        if not browse_path.exists():
            # Create the directory if it's under /app/data
            if str(browse_path).startswith('/app/data'):
                try:
                    browse_path.mkdir(parents=True, exist_ok=True)
                except Exception:
                    pass

            # If still doesn't exist, try parent directory
            if not browse_path.exists():
                browse_path = browse_path.parent

            # Last resort: use current working directory
            if not browse_path.exists():
                browse_path = Path.cwd()

        items = []

        if browse_type == "directory":
            # List directories only
            try:
                for item in sorted(browse_path.iterdir()):
                    if item.is_dir():
                        items.append({
                            "name": item.name,
                            "path": str(item),
                            "type": "directory"
                        })
            except PermissionError:
                return {
                    "success": False,
                    "message": "Permission denied to access this directory",
                    "items": []
                }

        elif browse_type == "database":
            # List .db, .sqlite, .sqlite3 files
            try:
                for item in sorted(browse_path.iterdir()):
                    if item.is_file() and item.suffix in ['.db', '.sqlite', '.sqlite3']:
                        items.append({
                            "name": item.name,
                            "path": str(item),
                            "type": "file"
                        })
                    elif item.is_dir():
                        # Also include directories for navigation
                        items.append({
                            "name": item.name,
                            "path": str(item),
                            "type": "directory"
                        })
            except PermissionError:
                return {
                    "success": False,
                    "message": "Permission denied to access this directory",
                    "items": []
                }

        # Add parent directory option if not at root
        if browse_path != browse_path.parent:
            items.insert(0, {
                "name": "..",
                "path": str(browse_path.parent),
                "type": "directory"
            })

        return {
            "success": True,
            "current_path": str(browse_path),
            "items": items,
            "message": f"Found {len(items)} items"
        }

    except Exception as e:
        logger = logging.getLogger(__name__)
        logger.exception(f"Error browsing filesystem: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error browsing filesystem: {str(e)}")


# ==================== REACT FRONTEND STATIC FILES ====================
# Mount React frontend LAST to ensure all API routes are registered first
# This prevents the static file mount from intercepting API requests

# In Docker: frontend build is copied to ./backend/static (same directory as main.py)
# In local dev: frontend build may be in ../static or ./static
static_dir = os.path.join(os.path.dirname(__file__), "static")
if not os.path.exists(static_dir):
    # Fallback for different directory structure
    static_dir = os.path.join(os.path.dirname(__file__), "..", "static")

if os.path.exists(static_dir):
    # Mount with html=True to serve index.html for all unmatched routes (SPA client-side routing)
    app.mount("/", StaticFiles(directory=static_dir, html=True), name="static")
    logger = logging.getLogger(__name__)
    logger.info(f"React frontend mounted at / from: {static_dir}")
else:
    logger = logging.getLogger(__name__)
    logger.warning(f"React frontend static directory not found: {static_dir}")
    logger.warning("React frontend will not be served. API-only mode.")
